{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Character-Level Transformer Training & Generation ðŸ¤–âœï¸\n",
        "\n",
        "This notebook implements a character-level Transformer model, trains it on the \"Tiny Shakespeare\" dataset, and then uses it to generate new text.\n",
        "\n",
        "### Key Features:\n",
        "* **Model**: A simplified implementation of a modern Transformer architecture, including features like RMS Normalization and Rotary Position Embeddings (RoPE).\n",
        "* **Training**: The model is trained from scratch to predict the next character in a sequence.\n",
        "* **Generation**: After training, the model can generate new text starting from a custom prompt, using a KV cache for efficient, token-by-token generation.\n",
        "\n",
        "## 1. Setup: Imports and Mock Functions\n",
        "\n",
        "First, we import all the necessary libraries.\n",
        "\n",
        "Since the original model code was designed for custom CUDA kernels for FP8 quantization, we'll create **mock functions**. These functions use standard PyTorch operations to ensure the code runs correctly in a standard environment like Colab, although without the performance benefits of the custom kernels.\n"
      ],
      "metadata": {
        "id": "YSqemAnopomE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Optional, Literal\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from torch.amp import autocast\n",
        "\n",
        "# This is a placeholder for the custom CUDA kernels.\n",
        "# Since we're running in a standard Python environment without compiling custom kernels,\n",
        "# we will mock these functions to use standard PyTorch operations.\n",
        "# NOTE: The performance will not match the original intent, but it will be functionally correct.\n",
        "def mock_act_quant(x, block_size):\n",
        "    return x, None # Pass-through for non-quantized training\n",
        "\n",
        "def mock_weight_dequant(weight, scale):\n",
        "    return weight # Pass-through\n",
        "\n",
        "def mock_fp8_gemm(x, scale_x, weight, scale_w):\n",
        "    # This simulates the GEMM operation using standard torch.matmul\n",
        "    return F.linear(x, weight)\n",
        "\n",
        "# Mock the kernel functions\n",
        "act_quant = mock_act_quant\n",
        "weight_dequant = mock_weight_dequant\n",
        "fp8_gemm = mock_fp8_gemm\n",
        "\n",
        "print(\"Imports and mock functions are ready.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports and mock functions are ready.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XI2EDMCpomG",
        "outputId": "1b1c3678-1d9a-45ca-b800-5fb74e134a4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Configuration\n",
        "\n",
        "Here, we define the global hyperparameters for training and model execution. You can modify these values to experiment with different settings."
      ],
      "metadata": {
        "id": "UecdU2t-pomH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Hyperparameters\n",
        "BATCH_SIZE = 64        # How many independent sequences will we process in parallel?\n",
        "BLOCK_SIZE = 256       # What is the maximum context length for predictions?\n",
        "MAX_ITERS = 500        # Total training iterations\n",
        "EVAL_INTERVAL = 100    # How often to evaluate and print loss\n",
        "LEARNING_RATE = 3e-4   # Optimizer learning rate\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "EVAL_ITERS = 50\n",
        "\n",
        "# Globals for model definition (will be set in the Transformer __init__)\n",
        "world_size = 1\n",
        "rank = 0\n",
        "# For training, we'll stick to bf16. fp8 training is more complex.\n",
        "gemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n",
        "# 'absorb' is the more memory-efficient attention implementation\n",
        "attn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\"\n",
        "# block_size for quantization (not critical for bf16 training)\n",
        "block_size = 128\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojmAplYOpomI",
        "outputId": "d747246d-689c-4019-f1bd-08f51a38592b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 3. Model Architecture\n",
        "\n",
        "This section contains the complete definition of our Transformer model. It's composed of several building blocks:\n",
        "\n",
        "* **`ModelArgs`**: A dataclass to hold all model hyperparameters.\n",
        "* **`RMSNorm`**: A modern normalization layer.\n",
        "* **`ParallelEmbedding`, `Linear`, etc.**: Custom linear and embedding layers.\n",
        "* **`MLA` (Multi-Query LoRA Attention)**: The attention mechanism.\n",
        "* **`MLP`**: The feed-forward network (part of the Transformer block).\n",
        "* **`Block`**: A single Transformer block combining attention and a feed-forward network.\n",
        "* **`Transformer`**: The main class that stacks the blocks together.\n",
        "\n",
        "A key optimization for generation is in the `Transformer.forward` method. During inference (`targets=None`), it calculates logits for **only the last token** (`h[:, [-1], :]`), which is much more efficient since we only need to predict the very next word.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XaViaaAWpomI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    \"\"\"Data class for defining model arguments and hyperparameters.\"\"\"\n",
        "    max_batch_size: int = 8\n",
        "    max_seq_len: int = 4096 * 4\n",
        "    dtype: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n",
        "    vocab_size: int = 102400\n",
        "    dim: int = 2048\n",
        "    inter_dim: int = 10944\n",
        "    moe_inter_dim: int = 1408\n",
        "    n_layers: int = 27\n",
        "    n_dense_layers: int = 1\n",
        "    n_heads: int = 16\n",
        "    n_routed_experts: int = 64\n",
        "    n_shared_experts: int = 2\n",
        "    n_activated_experts: int = 6\n",
        "    n_expert_groups: int = 1\n",
        "    n_limited_groups: int = 1\n",
        "    score_func: Literal[\"softmax\", \"sigmoid\"] = \"softmax\"\n",
        "    route_scale: float = 1.\n",
        "    q_lora_rank: int = 0\n",
        "    kv_lora_rank: int = 512\n",
        "    qk_nope_head_dim: int = 128\n",
        "    qk_rope_head_dim: int = 64\n",
        "    v_head_dim: int = 128\n",
        "    original_seq_len: int = 4096\n",
        "    rope_theta: float = 10000.0\n",
        "    rope_factor: float = 40\n",
        "    beta_fast: int = 32\n",
        "    beta_slow: int = 1\n",
        "    mscale: float = 1.\n",
        "\n",
        "\n",
        "class ParallelEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        assert vocab_size % world_size == 0, f\"Vocabulary size must be divisible by world size (world_size={world_size})\"\n",
        "        self.part_vocab_size = (vocab_size // world_size)\n",
        "        self.vocab_start_idx = rank * self.part_vocab_size\n",
        "        self.vocab_end_idx = self.vocab_start_idx + self.part_vocab_size\n",
        "        self.weight = nn.Parameter(torch.empty(self.part_vocab_size, self.dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if world_size > 1:\n",
        "            mask = (x < self.vocab_start_idx) | (x >= self.vocab_end_idx)\n",
        "            x = x - self.vocab_start_idx\n",
        "            x[mask] = 0\n",
        "        y = F.embedding(x, self.weight)\n",
        "        if world_size > 1:\n",
        "            y[mask] = 0.\n",
        "            dist.all_reduce(y)\n",
        "        return y\n",
        "\n",
        "def linear(x: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "    if weight.element_size() > 1 or gemm_impl == \"bf16\":\n",
        "        w = weight_dequant(weight, getattr(weight, 'scale', None)) if hasattr(weight, 'scale') else weight\n",
        "        if w.dtype != x.dtype:\n",
        "            w = w.to(x.dtype)\n",
        "        if bias is not None and bias.dtype != x.dtype:\n",
        "            bias = bias.to(x.dtype)\n",
        "        return F.linear(x, w, bias)\n",
        "    else:\n",
        "        x, scale = act_quant(x, block_size)\n",
        "        y = fp8_gemm(x, scale, weight, weight.scale)\n",
        "        if bias is not None:\n",
        "            y += bias\n",
        "        return y\n",
        "\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    dtype = torch.bfloat16\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype=None):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=dtype or Linear.dtype))\n",
        "        if self.weight.element_size() == 1:\n",
        "            scale_out_features = (out_features + block_size - 1) // block_size\n",
        "            scale_in_features = (in_features + block_size - 1) // block_size\n",
        "            self.weight.scale = self.scale = nn.Parameter(torch.empty(scale_out_features, scale_in_features, dtype=torch.float32))\n",
        "        else:\n",
        "            self.register_parameter(\"scale\", None)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.empty(out_features, dtype=dtype or Linear.dtype))\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return linear(x, self.weight, self.bias)\n",
        "\n",
        "class ColumnParallelLinear(Linear):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype=None):\n",
        "        assert out_features % world_size == 0\n",
        "        self.part_out_features = out_features // world_size\n",
        "        super().__init__(in_features, self.part_out_features, bias, dtype)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return linear(x, self.weight, self.bias)\n",
        "\n",
        "class RowParallelLinear(Linear):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype=None):\n",
        "        assert in_features % world_size == 0\n",
        "        self.part_in_features = in_features // world_size\n",
        "        super().__init__(self.part_in_features, out_features, bias, dtype)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y = linear(x, self.weight) # Bias is applied after all_reduce\n",
        "        if world_size > 1:\n",
        "            dist.all_reduce(y)\n",
        "        if self.bias is not None:\n",
        "            y += self.bias\n",
        "        return y\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        input_dtype = x.dtype\n",
        "        output = self._norm(x.float())\n",
        "        output = output * self.weight\n",
        "        return output.to(input_dtype)\n",
        "\n",
        "def precompute_freqs_cis(args: ModelArgs) -> torch.Tensor:\n",
        "    dim = args.qk_rope_head_dim\n",
        "    seqlen = args.max_seq_len\n",
        "    base = args.rope_theta\n",
        "    # ... (rest of precompute_freqs_cis is kept the same)\n",
        "    freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=DEVICE) / dim))\n",
        "    t = torch.arange(seqlen, device=DEVICE)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
        "    # Handle empty tensors gracefully\n",
        "    if x.numel() == 0:\n",
        "        return x\n",
        "    dtype = x.dtype\n",
        "    x = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2) # (1, seq, 1, dim/2)\n",
        "    y = torch.view_as_real(x * freqs_cis).flatten(3)\n",
        "    return y.to(dtype)\n",
        "\n",
        "class MLA(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.dim = args.dim\n",
        "        self.n_heads = args.n_heads\n",
        "        self.n_local_heads = args.n_heads // world_size\n",
        "        self.q_lora_rank = args.q_lora_rank\n",
        "        self.kv_lora_rank = args.kv_lora_rank\n",
        "        self.qk_nope_head_dim = args.qk_nope_head_dim\n",
        "        self.qk_rope_head_dim = args.qk_rope_head_dim\n",
        "        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim\n",
        "        self.v_head_dim = args.v_head_dim\n",
        "\n",
        "        if self.q_lora_rank == 0:\n",
        "            self.wq = ColumnParallelLinear(self.dim, self.n_heads * self.qk_head_dim, bias=False)\n",
        "        else:\n",
        "            self.wq_a = Linear(self.dim, self.q_lora_rank, bias=False)\n",
        "            self.q_norm = RMSNorm(self.q_lora_rank)\n",
        "            self.wq_b = ColumnParallelLinear(self.q_lora_rank, self.n_heads * self.qk_head_dim, bias=False)\n",
        "\n",
        "        self.wkv_a = Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim, bias=False)\n",
        "        self.kv_norm = RMSNorm(self.kv_lora_rank)\n",
        "        self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim), bias=False)\n",
        "        self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim, bias=False)\n",
        "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
        "\n",
        "        if attn_impl == \"naive\":\n",
        "            self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim, dtype=torch.bfloat16), persistent=False)\n",
        "            self.register_buffer(\"v_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim, dtype=torch.bfloat16), persistent=False)\n",
        "        else:\n",
        "            self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank, dtype=torch.bfloat16), persistent=False)\n",
        "            self.register_buffer(\"pe_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim, dtype=torch.bfloat16), persistent=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
        "        bsz, seqlen, _ = x.shape\n",
        "        end_pos = start_pos + seqlen\n",
        "\n",
        "        if self.q_lora_rank == 0:\n",
        "            q = self.wq(x)\n",
        "        else:\n",
        "            q = self.wq_b(self.q_norm(self.wq_a(x)))\n",
        "        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)\n",
        "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
        "\n",
        "        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
        "        kv = self.wkv_a(x)\n",
        "        kv, k_pe_new = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
        "        k_pe_new = apply_rotary_emb(k_pe_new.unsqueeze(2), freqs_cis).squeeze(2)\n",
        "\n",
        "        is_causal = start_pos == 0\n",
        "\n",
        "        if attn_impl == \"naive\":\n",
        "            q = torch.cat([q_nope, q_pe], dim=-1)\n",
        "            kv_b = self.wkv_b(self.kv_norm(kv))\n",
        "            kv_b = kv_b.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n",
        "            k_nope, v = torch.split(kv_b, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n",
        "            k_pe = k_pe_new.unsqueeze(2).expand(-1, -1, self.n_local_heads, -1)\n",
        "            k = torch.cat([k_nope, k_pe], dim=-1)\n",
        "            if not is_causal:\n",
        "                self.k_cache[range(bsz), start_pos:end_pos] = k\n",
        "                self.v_cache[range(bsz), start_pos:end_pos] = v\n",
        "                k = self.k_cache[range(bsz), :end_pos]\n",
        "                v = self.v_cache[range(bsz), :end_pos]\n",
        "            scores = torch.einsum(\"bshd,bthd->bsht\", q, k) * self.softmax_scale\n",
        "        else: # absorb implementation\n",
        "            wkv_b = self.wkv_b.weight if self.wkv_b.scale is None else weight_dequant(self.wkv_b.weight, self.wkv_b.scale)\n",
        "            wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank)\n",
        "            w = wkv_b[:, :self.qk_nope_head_dim].to(q_nope.dtype)\n",
        "            q_nope_c = torch.einsum(\"bshd,hdc->bshc\", q_nope, w)\n",
        "            kv_normed = self.kv_norm(kv)\n",
        "            if not is_causal:\n",
        "                self.kv_cache[range(bsz), start_pos:end_pos] = kv_normed\n",
        "                self.pe_cache[range(bsz), start_pos:end_pos] = k_pe_new\n",
        "                kv_cache_data = self.kv_cache[range(bsz), :end_pos]\n",
        "                pe_cache_data = self.pe_cache[range(bsz), :end_pos]\n",
        "            else:\n",
        "                kv_cache_data = kv_normed\n",
        "                pe_cache_data = k_pe_new\n",
        "            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope_c, kv_cache_data) +\n",
        "                      torch.einsum(\"bshr,btr->bsht\", q_pe, pe_cache_data)) * self.softmax_scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores += mask.unsqueeze(0).unsqueeze(2)\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(x)\n",
        "\n",
        "        if attn_impl == \"naive\":\n",
        "            output = torch.einsum(\"bsht,bthd->bshd\", scores, v)\n",
        "        else:\n",
        "            output = torch.einsum(\"bsht,btc->bshc\", scores, kv_cache_data)\n",
        "            w = wkv_b[:, -self.v_head_dim:].to(output.dtype)\n",
        "            output = torch.einsum(\"bshc,hdc->bshd\", output, w)\n",
        "\n",
        "        return self.wo(output.flatten(2))\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim: int, inter_dim: int):\n",
        "        super().__init__()\n",
        "        self.w1 = ColumnParallelLinear(dim, inter_dim, bias=False)\n",
        "        self.w2 = RowParallelLinear(inter_dim, dim, bias=False)\n",
        "        self.w3 = ColumnParallelLinear(dim, inter_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.attn = MLA(args)\n",
        "        self.ffn = MLP(args.dim, args.inter_dim)\n",
        "        self.attn_norm = RMSNorm(args.dim)\n",
        "        self.ffn_norm = RMSNorm(args.dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        h = x + self.attn(self.attn_norm(x), start_pos, freqs_cis, mask)\n",
        "        out = h + self.ffn(self.ffn_norm(h))\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        global world_size, rank\n",
        "        world_size = 1\n",
        "        rank = 0\n",
        "        self.max_seq_len = args.max_seq_len\n",
        "        self.embed = ParallelEmbedding(args.vocab_size, args.dim)\n",
        "        self.layers = torch.nn.ModuleList([Block(i, args) for i in range(args.n_layers)])\n",
        "        self.norm = RMSNorm(args.dim)\n",
        "        self.head = ColumnParallelLinear(args.dim, args.vocab_size, bias=False)\n",
        "        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(args).to(torch.bfloat16), persistent=False)\n",
        "        self.args = args\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (Linear, ColumnParallelLinear, RowParallelLinear, ParallelEmbedding)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if hasattr(module, 'bias') and module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, start_pos: int = 0, targets: Optional[torch.Tensor] = None):\n",
        "        seqlen = tokens.size(1)\n",
        "        h = self.embed(tokens)\n",
        "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
        "        mask = None\n",
        "        if seqlen > 1:\n",
        "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
        "            mask = torch.triu(mask, diagonal=1)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, start_pos, freqs_cis, mask)\n",
        "        h = self.norm(h)\n",
        "\n",
        "        if targets is not None:\n",
        "            # Training: compute logits for all tokens and calculate loss\n",
        "            logits = self.head(h)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        else:\n",
        "            # Inference: efficiently compute logits for the last token only\n",
        "            logits = self.head(h[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "print(\"Model architecture defined.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model architecture defined.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPIMpFg5pomI",
        "outputId": "6a038030-97d7-4a57-a52d-edfe67251ea5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Data Preparation\n",
        "\n",
        "We'll download the \"Tiny Shakespeare\" dataset, which is a small text file containing several of Shakespeare's works.\n",
        "\n",
        "We then perform character-level tokenization:\n",
        "1.  Find all unique characters in the text to create our vocabulary.\n",
        "2.  Create mappings from characters to integers (`stoi`) and integers back to characters (`itos`).\n",
        "3.  Define a helper function `get_batch` to randomly sample chunks of text for training and validation.\n"
      ],
      "metadata": {
        "id": "t4zvnnH3pomJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Tiny Shakespeare dataset if it doesn't exist\n",
        "if not os.path.exists('input.txt'):\n",
        "    print(\"Downloading Tiny Shakespeare dataset...\")\n",
        "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Character-level tokenization\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Split data into training and validation sets\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_source) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "    x = torch.stack([data_source[i:i+BLOCK_SIZE] for i in ix])\n",
        "    y = torch.stack([data_source[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    return x, y\n",
        "\n",
        "print(f\"Dataset loaded. Vocabulary size: {vocab_size}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Tiny Shakespeare dataset...\n",
            "--2025-08-17 10:56:39--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-08-17 10:56:39 (22.4 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n",
            "Dataset loaded. Vocabulary size: 65\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIjaUHEKpomJ",
        "outputId": "47093e10-b0e3-4377-b14b-fcc4d1f415df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Model Configuration for Training\n",
        "\n",
        "The default `ModelArgs` are for a large model. For this demo, we'll create a much smaller `TrainingModelArgs` configuration that can be trained quickly on a free Colab GPU.\n",
        "\n"
      ],
      "metadata": {
        "id": "navXiu8MpomJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainingModelArgs(ModelArgs):\n",
        "    # Drastically reduce parameters to fit on a free Colab GPU\n",
        "    max_batch_size: int = BATCH_SIZE\n",
        "    max_seq_len: int = BLOCK_SIZE\n",
        "    vocab_size: int = vocab_size\n",
        "    dim: int = 384\n",
        "    inter_dim: int = 768  # 2x dim\n",
        "    moe_inter_dim: int = 256\n",
        "    n_layers: int = 6\n",
        "    n_heads: int = 6\n",
        "    # Keep other params simple for this training run\n",
        "    n_routed_experts: int = 8\n",
        "    n_dense_layers: int = 6 # Use only MLP layers by setting this high\n",
        "    qk_nope_head_dim: int = 32\n",
        "    qk_rope_head_dim: int = 32\n",
        "    v_head_dim: int = 32\n",
        "    kv_lora_rank: int = 128\n",
        "    original_seq_len: int = BLOCK_SIZE\n",
        "\n",
        "print(\"Training-specific model arguments are set.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training-specific model arguments are set.\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nycKJD_6pomJ",
        "outputId": "ebe24b35-401d-4403-dc41-285843b16ec0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 6. Training Setup\n",
        "\n",
        "This section defines the helper functions for our training loop:\n",
        "* **`estimate_loss`**: Evaluates the model's performance on the training and validation sets without updating weights.\n",
        "* **`save_checkpoint` / `load_checkpoint`**: Functions to save and resume training progress. This is useful in case the Colab instance disconnects.\n",
        "* **`train`**: The main training function that orchestrates the entire process.\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "### **Cell 13: Code**"
      ],
      "metadata": {
        "id": "uTgju0sWpomK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(EVAL_ITERS)\n",
        "        for k in range(EVAL_ITERS):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, targets=Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "CHECKPOINT_PATH = \"shakespeare_checkpoint.pth\"\n",
        "\n",
        "def save_checkpoint(model, optimizer, iter_num, args):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'iter_num': iter_num,\n",
        "        'args': args,\n",
        "    }, CHECKPOINT_PATH)\n",
        "\n",
        "def load_checkpoint():\n",
        "    if not torch.cuda.is_available():\n",
        "        map_location = torch.device(\"cpu\")\n",
        "    else:\n",
        "        map_location = DEVICE\n",
        "\n",
        "    if os.path.exists(CHECKPOINT_PATH):\n",
        "        print(\"Resuming from checkpoint...\")\n",
        "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=map_location)\n",
        "        return checkpoint\n",
        "    return None\n",
        "\n",
        "def train():\n",
        "    torch.manual_seed(1337)\n",
        "\n",
        "    args = TrainingModelArgs()\n",
        "    model = Transformer(args)\n",
        "    model.to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Try to load checkpoint\n",
        "    start_iter = 0\n",
        "    checkpoint = load_checkpoint()\n",
        "    if checkpoint:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_iter = checkpoint['iter_num'] + 1\n",
        "        print(f\"Checkpoint loaded. Resuming from iteration {start_iter}.\")\n",
        "\n",
        "    print(f\"Model on {DEVICE}. Total params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "    pbar = tqdm(range(start_iter, MAX_ITERS), desc=\"Training\")\n",
        "    for iter_num in pbar:\n",
        "        if iter_num % EVAL_INTERVAL == 0 or iter_num == MAX_ITERS - 1:\n",
        "            losses = estimate_loss(model)\n",
        "            pbar.set_postfix({\n",
        "                \"train_loss\": f\"{losses['train']:.4f}\",\n",
        "                \"val_loss\": f\"{losses['val']:.4f}\"\n",
        "            })\n",
        "            save_checkpoint(model, optimizer, iter_num, args)\n",
        "\n",
        "        xb, yb = get_batch('train')\n",
        "        with autocast(device_type=DEVICE, dtype=torch.bfloat16, enabled=(DEVICE == 'cuda')):\n",
        "            logits, loss = model(xb, targets=yb)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    print(\"\\nTraining complete.\")\n",
        "    final_losses = estimate_loss(model)\n",
        "    print(f\"Final train loss: {final_losses['train']:.4f}, val loss: {final_losses['val']:.4f}\")\n",
        "\n",
        "    print(\"Saving final model...\")\n",
        "    torch.save(model.state_dict(), 'shakespeare_model.pth')\n",
        "    return args\n",
        "\n",
        "print(\"Training functions are ready.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training functions are ready.\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBh4MJtlpomK",
        "outputId": "25b99350-3e34-4ccc-d054-4f3baa7881dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Cell 14: Markdown**\n",
        "\n",
        "\n",
        "## 7. Text Generation Function\n",
        "\n",
        "The `generate` function takes the trained model and a starting prompt to produce new text. It uses several techniques for high-quality sampling:\n",
        "* **Temperature Scaling**: Controls the randomness of predictions.\n",
        "* **Top-k Sampling**: Limits the sampling pool to the `k` most likely next tokens.\n",
        "* **Top-p (Nucleus) Sampling**: Selects from the smallest set of tokens whose cumulative probability exceeds `p`.\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "### **Cell 15: Code**"
      ],
      "metadata": {
        "id": "Tuq1UMkmpomK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model_args, prompt: str = \"\\n\", max_new_tokens: int = 500, temperature: float = 0.8, top_k: int = 200, top_p: Optional[float] = None):\n",
        "    \"\"\"Generates text from a trained Transformer model using KV-cache.\"\"\"\n",
        "\n",
        "    print(\"\\nLoading model for generation...\")\n",
        "    torch.set_default_dtype(torch.bfloat16)\n",
        "\n",
        "    # Cap max_new_tokens to avoid running past the model's configured sequence length\n",
        "    if max_new_tokens + len(prompt) > model_args.max_seq_len:\n",
        "        new_len = model_args.max_seq_len - len(prompt)\n",
        "        print(f\"Warning: Prompt length + max_new_tokens exceeds model's max_seq_len ({model_args.max_seq_len}).\")\n",
        "        print(f\"Capping generation at {new_len} tokens.\")\n",
        "        max_new_tokens = new_len\n",
        "\n",
        "    model = Transformer(model_args)\n",
        "    model.load_state_dict(torch.load('shakespeare_model.pth', map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Generating text...\")\n",
        "    start_ids = encode(prompt)\n",
        "    idx = torch.tensor(start_ids, dtype=torch.long, device=DEVICE)[None, ...]\n",
        "\n",
        "    # Print the starting prompt first\n",
        "    print(prompt, end='', flush=True)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "        # get the predictions\n",
        "        logits, _ = model(idx_cond)\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # apply top-k filtering\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "        # apply top-p (nucleus) filtering\n",
        "        if top_p is not None and 0.0 < top_p < 1.0:\n",
        "            probs_sort, indices_sort = torch.sort(logits, descending=True)\n",
        "            probs_sum = torch.cumsum(F.softmax(probs_sort, dim=-1), dim=-1)\n",
        "            mask = probs_sum > top_p\n",
        "            mask[:, 1:] = mask[:, :-1].clone()\n",
        "            mask[:, 0] = 0\n",
        "            indices_to_remove = mask.scatter(1, indices_sort, mask)\n",
        "            logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "        # sample from the distribution\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        # append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        print(decode(idx_next[0].tolist()), end='', flush=True)\n",
        "\n",
        "    print(\"\\n--- Generation Complete ---\")\n",
        "\n",
        "print(\"Generation function is ready.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation function is ready.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1uwuucypomK",
        "outputId": "ffe38766-6f9b-4a71-e944-1dbc7edf7d39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 8. Run Training and Generation! ðŸš€\n",
        "\n",
        "This is the final step. We'll call the `train()` function to start training the model. Once training is complete, the `generate()` function is called with your custom prompt.\n",
        "\n"
      ],
      "metadata": {
        "id": "5_kv-0ujpomK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    trained_model_args = train()\n",
        "    generate(trained_model_args, prompt=\"hello thee\", max_new_tokens=500)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1393756360.py:280: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:308.)\n",
            "  self.register_buffer(\"freqs_cis\", precompute_freqs_cis(args).to(torch.bfloat16), persistent=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on cuda. Total params: 7.35M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [05:51<00:00,  1.42it/s, train_loss=1.3823, val_loss=1.5896]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete.\n",
            "Final train loss: 1.3792, val loss: 1.5903\n",
            "Saving final model...\n",
            "\n",
            "Loading model for generation...\n",
            "Warning: Prompt length + max_new_tokens exceeds model's max_seq_len (256).\n",
            "Capping generation at 246 tokens.\n",
            "Generating text...\n",
            "hello thee wind affection.\n",
            "\n",
            "STANLEY:\n",
            "He made sometimes encounter my counterfellow\n",
            "First Squestion? why lady, end you my father,\n",
            "And I make the mooft the both all dill,\n",
            "Ses it ender commfort: there by the commend.\n",
            "\n",
            "First Soldier:\n",
            "The earthere my could speak\n",
            "--- Generation Complete ---\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-p7vYhZpomK",
        "outputId": "6fc1032e-d8b8-4117-f0db-b5bf8c9f4849"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}